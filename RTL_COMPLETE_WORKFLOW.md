# GraphCodeBERT-RTL å®Œæ•´æµç¨‹æ–‡æ¡£
# Complete Workflow Documentation for GraphCodeBERT-RTL

## æ¦‚è¿° (Overview)

æœ¬æ–‡æ¡£è¯¦ç»†æè¿°äº†é’ˆå¯¹RTLæ¨¡å—åœ¨GraphCodeBERT-RTLé¡¹ç›®ä¸­çš„å®Œæ•´ä»£ç è¿è¡Œæµç¨‹ï¼ŒåŒ…æ‹¬ä»ä»£ç è½¬æ¢ä¸ºDFGã€ç”ŸæˆMijçŸ©é˜µã€èåˆtransformerã€å®Œæˆé¢„è®­ç»ƒï¼Œåˆ°æœ€ç»ˆæµ‹è¯•å’Œè¾“å‡ºçš„å®Œæ•´è¿‡ç¨‹ã€‚

**This document provides a detailed description of the complete code execution workflow for RTL modules in the GraphCodeBERT-RTL project, covering the entire process from code conversion to DFG, Mij matrix generation, transformer fusion, pretraining completion, to final testing and output.**

---

## å®Œæ•´æµç¨‹æ¦‚è§ˆ (Complete Workflow Overview)

```
RTL Verilogä»£ç  + æ³¨é‡Š 
    â†“
1. DFGè½¬æ¢ (parser/DFG.py)
    â†“
2. MijçŸ©é˜µç”Ÿæˆ (error_correction_model.py)
    â†“ 
3. Transformerèåˆ (error_correction_model.py)
    â†“
4. é¢„è®­ç»ƒ (rtl_error_correction.py)
    â†“
5. æµ‹è¯•å’Œè¾“å‡º (demo_rtl_error_correction.py, test_offline.py)
```

---

## 1. DFG (æ•°æ®æµå›¾) è½¬æ¢é˜¶æ®µ
### DFG (Data Flow Graph) Conversion Stage

**æ–‡ä»¶ä½ç½®**: `GraphCodeBERT/rtl_error_localization/parser/DFG.py`
**å…³é”®å‡½æ•°**: `DFG_verilog(root_node, index_to_code, states)` (ç¬¬1184-1347è¡Œ)

### åŠŸèƒ½æè¿° (Functionality Description)

è¿™ä¸ªé˜¶æ®µå°†Verilog RTLä»£ç è½¬æ¢ä¸ºæ•°æ®æµå›¾ï¼Œæå–å˜é‡ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚

**This stage converts Verilog RTL code into data flow graphs, extracting dependencies between variables.**

### å¤„ç†çš„Verilogç»“æ„ (Processed Verilog Structures)

```python
# æ”¯æŒçš„Verilogè¯­æ³•ç»“æ„
assignment = ['continuous_assign', 'blocking_assignment', 'nonblocking_assignment']
variable_declaration = ['net_declaration', 'variable_declaration', 'port_declaration'] 
always_block = ['always_construct', 'initial_construct']
if_statement = ['conditional_statement']
for_statement = ['loop_statement']
case_statement = ['case_statement']
module_instantiation = ['module_instantiation']
```

### DFGè¾“å‡ºæ ¼å¼ (DFG Output Format)

```python
# DFGå…ƒç»„æ ¼å¼: (å˜é‡å, ç´¢å¼•, å…³ç³»ç±»å‹, ä¾èµ–å˜é‡åˆ—è¡¨, ä¾èµ–ç´¢å¼•åˆ—è¡¨)
DFG_node = (variable_name, index, relationship_type, dependency_variables, dependency_indices)
# ä¾‹å¦‚: ('b', 1, 'computedFrom', ['a'], [0])
```

### ç¤ºä¾‹è½¬æ¢ (Example Conversion)

```verilog
// è¾“å…¥RTLä»£ç 
module test(input a, output b);
    assign b = a;
endmodule

// ç”Ÿæˆçš„DFG
[('a', 0, 'comesFrom', [], []),
 ('b', 1, 'computedFrom', ['a'], [0])]
```

---

## 2. MijçŸ©é˜µç”Ÿæˆé˜¶æ®µ
### Mij Matrix Generation Stage

**æ–‡ä»¶ä½ç½®**: `GraphCodeBERT/rtl_error_localization/error_correction_model.py`
**å…³é”®ä»£ç **: ç¬¬78-82è¡Œ

### åŠŸèƒ½æè¿° (Functionality Description)

MijçŸ©é˜µæ˜¯GraphCodeBERTæ¶æ„çš„æ ¸å¿ƒç»„ä»¶ï¼Œç”¨äºèåˆDFGä¿¡æ¯ä¸ä»£ç æ ‡è®°ï¼Œå®ç°å¤šæ¨¡æ€ä¿¡æ¯çš„æœ‰æ•ˆæ•´åˆã€‚

**The Mij matrix is a core component of the GraphCodeBERT architecture, used to fuse DFG information with code tokens, achieving effective integration of multimodal information.**

### MijçŸ©é˜µè®¡ç®—è¿‡ç¨‹ (Mij Matrix Calculation Process)

```python
def forward(self, source_ids, source_mask, position_idx, attn_mask, target_ids=None, target_mask=None, args=None):
    # 1. è¯†åˆ«DFGèŠ‚ç‚¹å’Œä»£ç æ ‡è®°
    nodes_mask = position_idx.eq(0)  # DFGèŠ‚ç‚¹ä½ç½®ç¼–ç ä¸º0
    token_mask = position_idx.ge(2)  # ä»£ç æ ‡è®°ä½ç½®ç¼–ç >=2
    
    # 2. ç”Ÿæˆè¯åµŒå…¥
    inputs_embeddings = self.encoder.embeddings.word_embeddings(source_ids)
    
    # 3. è®¡ç®—MijçŸ©é˜µ - DFGèŠ‚ç‚¹ä¸ä»£ç æ ‡è®°çš„å…³è”åº¦
    nodes_to_token_mask = nodes_mask[:,:,None] & token_mask[:,None,:] & attn_mask.bool()
    nodes_to_token_mask = nodes_to_token_mask.float() / (nodes_to_token_mask.sum(-1, keepdim=True).float() + 1e-10)
    
    # 4. ä½¿ç”¨MijçŸ©é˜µè¿›è¡ŒåŠ æƒèåˆ
    avg_embeddings = torch.einsum("abc,acd->abd", nodes_to_token_mask, inputs_embeddings)
    inputs_embeddings = inputs_embeddings * (~nodes_mask)[:,:,None].float() + avg_embeddings * nodes_mask[:,:,None].float()
```

### ä½ç½®ç¼–ç ç­–ç•¥ (Position Encoding Strategy)

```python
# åŸºäºMijçŸ©é˜µçš„å¤šæ¨¡æ€ä½ç½®ç¼–ç 
position_idx = []
for i in range(len(source_tokens)):
    if i < dfg_start:
        if i >= code_start:
            position_idx.append(i - code_start + 2)  # ä»£ç æ ‡è®°: >=2
        else:
            position_idx.append(1)  # æ³¨é‡Šå†…å®¹: =1
    else:
        position_idx.append(0)  # DFGèŠ‚ç‚¹: =0
```

---

## 3. Transformerèåˆé˜¶æ®µ
### Transformer Fusion Stage

**æ–‡ä»¶ä½ç½®**: `GraphCodeBERT/rtl_error_localization/error_correction_model.py`
**å…³é”®ä»£ç **: ç¬¬84-89è¡Œ

### åŠŸèƒ½æè¿° (Functionality Description)

åœ¨è¿™ä¸ªé˜¶æ®µï¼Œèåˆäº†DFGä¿¡æ¯çš„åµŒå…¥å‘é‡è¢«è¾“å…¥åˆ°transformerç¼–ç å™¨ä¸­ï¼ŒåŒæ—¶è®¡ç®—é”™è¯¯ç½®ä¿¡åº¦åˆ†æ•°ã€‚

**In this stage, embeddings fused with DFG information are fed into the transformer encoder, while calculating error confidence scores.**

### Transformerç¼–ç è¿‡ç¨‹ (Transformer Encoding Process)

```python
# 1. ä½¿ç”¨èåˆåçš„åµŒå…¥è¿›è¡Œç¼–ç 
outputs = self.encoder(inputs_embeds=inputs_embeddings, 
                      attention_mask=attn_mask, 
                      position_ids=position_idx)
encoder_output = outputs[0].permute([1,0,2]).contiguous()

# 2. è®¡ç®—æ¯ä¸ªæ ‡è®°çš„é”™è¯¯ç½®ä¿¡åº¦åˆ†æ•°
error_scores = self.sigmoid(self.error_confidence(encoder_output.permute([1,0,2])))
```

### æ¨¡å‹æ¶æ„ (Model Architecture)

```python
class RTLErrorCorrectionModel(nn.Module):
    def __init__(self, encoder, decoder, config, beam_size, max_length, sos_id, eos_id):
        super(RTLErrorCorrectionModel, self).__init__()
        self.encoder = encoder          # RoBERTaç¼–ç å™¨
        self.decoder = decoder          # Transformerè§£ç å™¨
        self.error_confidence = nn.Linear(config.hidden_size, 1)  # é”™è¯¯æ£€æµ‹å¤´
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)  # å¯†é›†å±‚
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)  # è¯­è¨€æ¨¡å‹å¤´
```

---

## 4. é¢„è®­ç»ƒé˜¶æ®µ
### Pretraining Stage

**æ–‡ä»¶ä½ç½®**: `GraphCodeBERT/rtl_error_localization/rtl_error_correction.py`
**å…³é”®å‡½æ•°**: `main()` å‡½æ•°ä¸­çš„è®­ç»ƒå¾ªç¯

### åŠŸèƒ½æè¿° (Functionality Description)

é¢„è®­ç»ƒé˜¶æ®µä½¿ç”¨æ­£ç¡®çš„RTLä»£ç ã€æ³¨é‡Šå’ŒDFGä¿¡æ¯æ¥è®­ç»ƒæ¨¡å‹ï¼Œå­¦ä¹ RTLä»£ç çš„æ­£ç¡®æ¨¡å¼å’Œé”™è¯¯æ£€æµ‹èƒ½åŠ›ã€‚

**The pretraining stage uses correct RTL code, comments, and DFG information to train the model, learning correct patterns of RTL code and error detection capabilities.**

### é¢„è®­ç»ƒæ•°æ®æ ¼å¼ (Pretraining Data Format)

```python
# é¢„è®­ç»ƒæ ·æœ¬ç»“æ„
{
    'input': {
        'code': 'module test(input a, output b); assign b = a; endmodule',
        'comments': 'Simple wire connection module',
        'dfg_nodes': ['a', 'b', 'assign'],
        'dfg_edges': [('b', 'computedFrom', ['a'])]
    },
    'target': {
        'type': 'pretraining',
        'masked_prediction': True
    }
}
```

### è®­ç»ƒè¿‡ç¨‹ (Training Process)

```python
def main():
    # 1. åŠ è½½é¢„è®­ç»ƒé…ç½®å’Œæ¨¡å‹
    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]
    config = config_class.from_pretrained(args.model_name_or_path)
    tokenizer = tokenizer_class.from_pretrained(args.model_name_or_path)
    
    # 2. åˆ›å»ºRTLé”™è¯¯ä¿®æ­£æ¨¡å‹
    encoder = model_class.from_pretrained(args.model_name_or_path, config=config)
    decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)
    model = RTLErrorCorrectionModel(encoder, decoder, config, args.beam_size, 
                                  args.max_target_length, tokenizer.cls_token_id, tokenizer.sep_token_id)
    
    # 3. è®­ç»ƒå¾ªç¯
    for epoch in range(args.num_train_epochs):
        model.train()
        for batch in train_dataloader:
            # å‰å‘ä¼ æ’­
            outputs = model(source_ids=batch[0], source_mask=batch[1], 
                          position_idx=batch[2], attn_mask=batch[3],
                          target_ids=batch[4], target_mask=batch[5])
            loss = outputs[0]
            
            # åå‘ä¼ æ’­å’Œä¼˜åŒ–
            loss.backward()
            optimizer.step()
            scheduler.step()
            optimizer.zero_grad()
```

### æŸå¤±å‡½æ•° (Loss Function)

```python
# åºåˆ—åˆ°åºåˆ—çš„äº¤å‰ç†µæŸå¤±
loss_fct = nn.CrossEntropyLoss(ignore_index=-1)
correction_loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1))[active_loss],
                         shift_labels.view(-1)[active_loss])
```

---

## 5. æµ‹è¯•å’Œè¾“å‡ºé˜¶æ®µ
### Testing and Output Stage

**ä¸»è¦æ–‡ä»¶**: 
- `GraphCodeBERT/rtl_error_localization/demo_rtl_error_correction.py` (æ¼”ç¤ºå’Œæµ‹è¯•)
- `GraphCodeBERT/rtl_error_localization/test_offline.py` (ç¦»çº¿æµ‹è¯•)

### åŠŸèƒ½æè¿° (Functionality Description)

æµ‹è¯•é˜¶æ®µè¾“å…¥æœ‰ç¼ºé™·çš„RTLä»£ç ï¼Œæ¨¡å‹è¾“å‡ºç¼ºé™·ä½ç½®å’Œä¿®æ­£åçš„ä»£ç ã€‚

**The testing stage inputs defective RTL code, and the model outputs defect locations and corrected code.**

### æµ‹è¯•æµç¨‹ (Testing Workflow)

#### 5.1 æ¼”ç¤ºæµ‹è¯• (Demo Testing)

**æ–‡ä»¶**: `demo_rtl_error_correction.py`
**å…³é”®å‡½æ•°**: `demonstrate_testing_workflow()`

```python
def demonstrate_testing_workflow(system):
    """æ¼”ç¤ºæµ‹è¯•å·¥ä½œæµç¨‹ï¼Œè¾“å…¥æœ‰ç¼ºé™·çš„ä»£ç ï¼Œè¾“å‡ºç¼ºé™·ä½ç½®å’Œä¿®æ­£ä»£ç """
    
    # æµ‹è¯•ç”¨ä¾‹ï¼šæœ‰ç¼ºé™·çš„RTLä»£ç 
    defective_examples = [
        {
            'code': '''module test(input a, output b);
    assign b = a + 1;  // é”™è¯¯ï¼šä¸å¿…è¦çš„ç®—æœ¯è¿ç®—
endmodule''',
            'description': 'Unnecessary arithmetic operation'
        },
        {
            'code': '''module logic_or(input a, b, output c);
    assign c = a & b;  // é”™è¯¯ï¼šåº”è¯¥æ˜¯ORè€Œä¸æ˜¯AND
endmodule''',
            'description': 'Wrong logic operation (should be OR)'
        }
    ]
    
    # å¯¹æ¯ä¸ªæµ‹è¯•ç”¨ä¾‹è¿›è¡Œåˆ†æ
    for i, example in enumerate(defective_examples):
        result = system.analyze_defective_code(example['code'])
        
        # è¾“å‡ºç»“æœ
        print(f"âœ… æ£€æµ‹åˆ° {len(result['defect_locations'])} ä¸ªç¼ºé™·")
        print(f"ğŸ“ ç¼ºé™·ä½ç½®: {result['defect_locations']}")
        print(f"ğŸ”§ ä¿®æ­£åä»£ç : {result['corrected_code']}")
```

#### 5.2 ç¦»çº¿æµ‹è¯• (Offline Testing)

**æ–‡ä»¶**: `test_offline.py`

```python
# æ‰¹é‡æµ‹è¯•å’Œè¯„ä¼°
def test_model_offline():
    """ç¦»çº¿æµ‹è¯•æ¨¡å‹æ€§èƒ½"""
    
    # åŠ è½½æµ‹è¯•æ•°æ®é›†
    test_dataset = load_test_dataset()
    
    # æ¨¡å‹æ¨ç†
    model.eval()
    predictions = []
    
    for batch in test_dataloader:
        with torch.no_grad():
            # ç”Ÿæˆä¿®æ­£ä»£ç 
            preds = model(source_ids=batch[0], source_mask=batch[1], 
                         position_idx=batch[2], attn_mask=batch[3])
            
            # è§£ç é¢„æµ‹ç»“æœ
            for pred in preds:
                corrected_code = tokenizer.decode(pred, skip_special_tokens=True)
                predictions.append(corrected_code)
    
    # è¯„ä¼°æŒ‡æ ‡è®¡ç®—
    bleu_score = calculate_bleu(predictions, references)
    accuracy = calculate_accuracy(predictions, references)
    
    return {
        'bleu_score': bleu_score,
        'accuracy': accuracy,
        'predictions': predictions
    }
```

### è¾“å‡ºæ ¼å¼ (Output Format)

```python
# æµ‹è¯•è¾“å‡ºç»“æœæ ¼å¼
{
    'defect_locations': [
        {
            'line': 1,
            'column_start': 42,
            'column_end': 45,
            'type': 'unnecessary_arithmetic',
            'severity': 'high',
            'description': 'Unnecessary arithmetic operation detected'
        }
    ],
    'corrected_code': 'module test(input a, output b); assign b = a; endmodule',
    'confidence_score': 0.95,
    'modification_summary': ['Removed unnecessary arithmetic operation']
}
```

---

## 6. æ–‡ä»¶åŠŸèƒ½æ€»ç»“ (File Function Summary)

### æ ¸å¿ƒå¤„ç†æ–‡ä»¶ (Core Processing Files)

| æ–‡ä»¶è·¯å¾„ | åŠŸèƒ½ | å…³é”®ç»„ä»¶ |
|---------|------|----------|
| `parser/DFG.py` | **DFGè½¬æ¢** | `DFG_verilog()` å‡½æ•° |
| `error_correction_model.py` | **MijçŸ©é˜µç”Ÿæˆ + Transformerèåˆ** | `RTLErrorCorrectionModel` ç±» |
| `rtl_error_correction.py` | **é¢„è®­ç»ƒä¸»ç¨‹åº** | `main()` è®­ç»ƒå¾ªç¯ |
| `demo_rtl_error_correction.py` | **æ¼”ç¤ºæµ‹è¯•** | `demonstrate_testing_workflow()` |
| `test_offline.py` | **ç¦»çº¿æµ‹è¯•å’Œè¯„ä¼°** | æ‰¹é‡æµ‹è¯•å‡½æ•° |

### è¾…åŠ©æ–‡ä»¶ (Supporting Files)

| æ–‡ä»¶è·¯å¾„ | åŠŸèƒ½ | è¯´æ˜ |
|---------|------|------|
| `run.py` | **é€šç”¨è®­ç»ƒè„šæœ¬** | å¯é…ç½®çš„è®­ç»ƒå’Œæµ‹è¯•å…¥å£ |
| `model.py` | **åŸºç¡€æ¨¡å‹å®šä¹‰** | Seq2SeqåŸºç¡€æ¶æ„ |
| `parser/utils.py` | **è§£æå·¥å…·** | ä»£ç é¢„å¤„ç†å’Œæ ‡è®°åŒ– |

---

## 7. è¿è¡Œå‘½ä»¤ç¤ºä¾‹ (Execution Command Examples)

### é¢„è®­ç»ƒ (Pretraining)

```bash
# è¿è¡Œé¢„è®­ç»ƒ
python rtl_error_correction.py \
    --model_type roberta \
    --model_name_or_path microsoft/codebert-base \
    --do_train \
    --train_data_file train.json \
    --output_dir ./output \
    --max_source_length 512 \
    --max_target_length 512 \
    --beam_size 10 \
    --train_batch_size 8 \
    --eval_batch_size 8 \
    --learning_rate 5e-5 \
    --num_train_epochs 10
```

### æµ‹è¯• (Testing)

```bash
# è¿è¡Œæ¼”ç¤ºæµ‹è¯•
python demo_rtl_error_correction.py

# è¿è¡Œç¦»çº¿æµ‹è¯•
python test_offline.py \
    --model_path ./output/pytorch_model.bin \
    --test_data_file test.json \
    --output_file results.json
```

---

## 8. æ•°æ®æµå›¾ (Data Flow Diagram)

```
è¾“å…¥: RTLä»£ç  + æ³¨é‡Š
    â†“
[parser/DFG.py] 
    â†’ DFG_verilog() â†’ æ•°æ®æµå›¾
    â†“
[error_correction_model.py]
    â†’ MijçŸ©é˜µè®¡ç®— â†’ å¤šæ¨¡æ€èåˆ
    â†“
[error_correction_model.py] 
    â†’ Transformerç¼–ç å™¨ â†’ ä¸Šä¸‹æ–‡è¡¨ç¤º
    â†“
[rtl_error_correction.py]
    â†’ é¢„è®­ç»ƒå¾ªç¯ â†’ è®­ç»ƒå¥½çš„æ¨¡å‹
    â†“
[demo_rtl_error_correction.py / test_offline.py]
    â†’ æ¨ç†å’Œæµ‹è¯• â†’ ç¼ºé™·ä½ç½® + ä¿®æ­£ä»£ç 
```

---

## 9. æŠ€æœ¯ç‰¹ç‚¹ (Technical Features)

### å¤šæ¨¡æ€èåˆ (Multimodal Fusion)
- **DFGä¿¡æ¯**: æä¾›ä»£ç çš„ç»“æ„åŒ–ä¾èµ–å…³ç³»
- **ä»£ç æ ‡è®°**: ä¿æŒåŸå§‹ä»£ç çš„è¯­æ³•ä¿¡æ¯  
- **æ³¨é‡Šä¿¡æ¯**: å¢å¼ºè¯­ä¹‰ç†è§£

### MijçŸ©é˜µä¼˜åŠ¿ (Mij Matrix Advantages)
- **ç²¾ç¡®èåˆ**: å‡†ç¡®è®¡ç®—DFGèŠ‚ç‚¹ä¸ä»£ç æ ‡è®°çš„å…³è”
- **ä¿æŒå…¼å®¹**: ä¸åŸå§‹GraphCodeBERTæ¶æ„å®Œå…¨å…¼å®¹
- **é«˜æ•ˆè®¡ç®—**: ä½¿ç”¨einsumè¿›è¡Œé«˜æ•ˆçš„å¼ é‡è¿ç®—

### RTLç‰¹åŒ– (RTL Specialization)  
- **Verilogè¯­æ³•æ”¯æŒ**: å®Œæ•´æ”¯æŒVerilog/SystemVerilogè¯­æ³•
- **ç¡¬ä»¶è¯­ä¹‰**: ç†è§£RTLçš„ç¡¬ä»¶æè¿°è¯­ä¹‰
- **é”™è¯¯æ¨¡å¼**: ä¸“é—¨é’ˆå¯¹RTLå¸¸è§é”™è¯¯è¿›è¡Œè®­ç»ƒ

---

é€šè¿‡ä»¥ä¸Šå®Œæ•´çš„æµç¨‹æ–‡æ¡£ï¼Œæ‚¨å¯ä»¥æ¸…æ¥šåœ°äº†è§£GraphCodeBERT-RTLé¡¹ç›®ä¸­æ¯ä¸ªé˜¶æ®µçš„å…·ä½“å®ç°æ–‡ä»¶å’ŒåŠŸèƒ½ï¼Œä»¥åŠæ•´ä¸ªç³»ç»Ÿæ˜¯å¦‚ä½•ååŒå·¥ä½œæ¥å®ç°RTLä»£ç é”™è¯¯æ£€æµ‹å’Œä¿®æ­£çš„ã€‚

**Through this complete workflow documentation, you can clearly understand the specific implementation files and functions of each stage in the GraphCodeBERT-RTL project, and how the entire system works together to achieve RTL code error detection and correction.**